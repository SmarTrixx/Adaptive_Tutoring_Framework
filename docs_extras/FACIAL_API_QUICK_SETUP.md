# FACIAL EXPRESSION API - QUICK SETUP

## STATUS: ‚úÖ READY FOR DEPLOYMENT

**Backend:** ‚úÖ Complete - 4 new endpoints added  
**Frontend:** ‚è≥ Needs integration - Follow steps below

---

## 3-STEP QUICK START

### Step 1: Update Frontend HTML
Add this to `frontend/index.html` inside the `<body>` tag:

```html
<!-- Facial Monitoring Panel -->
<div id="facial-monitoring-container" style="position: fixed; top: 20px; right: 20px; display: none; background: white; padding: 15px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.15); z-index: 1000;">
    <div style="font-size: 12px; font-weight: 600; color: #333; margin-bottom: 8px;">
        üìπ Facial Monitoring
    </div>
    <div style="font-size: 11px; color: #666; margin-bottom: 8px;">
        Current Emotion: <strong id="emotion-display">--</strong>
    </div>
    <video id="webcam-video" autoplay playsinline muted 
           style="width: 160px; height: 120px; border: 1px solid #ddd; border-radius: 4px; margin-bottom: 8px;"></video>
    <div style="font-size: 10px; color: #999;">
        <input type="checkbox" id="facial-enabled"> Enable facial capture
    </div>
</div>
```

### Step 2: Update Frontend JavaScript
Add this to `frontend/app.js` at the end of the file:

```javascript
// ============================================================================
// FACIAL EXPRESSION MONITORING
// ============================================================================

let facialCapture = null;
let facialMonitoringEnabled = false;

class WebcamFacialCapture {
    constructor(videoElementId) {
        this.video = document.getElementById(videoElementId);
        this.stream = null;
        this.isRunning = false;
        this.detectionInterval = null;
        this.modelsLoaded = false;
    }
    
    async loadModels() {
        if (this.modelsLoaded) return;
        console.log('Loading facial detection models...');
        
        try {
            await Promise.all([
                faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/'),
                faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/')
            ]);
            
            this.modelsLoaded = true;
            console.log('‚úÖ Facial detection models loaded');
        } catch (error) {
            console.error('‚ùå Failed to load facial models:', error);
            throw error;
        }
    }
    
    async initialize() {
        try {
            // Load models first
            await this.loadModels();
            
            // Request camera access
            this.stream = await navigator.mediaDevices.getUserMedia({ 
                video: { 
                    width: { ideal: 640 },
                    height: { ideal: 480 },
                    facingMode: 'user'
                },
                audio: false 
            });
            
            this.video.srcObject = this.stream;
            this.video.onloadedmetadata = () => {
                this.video.play().catch(e => console.error('Video play error:', e));
            };
            
            console.log('‚úÖ Webcam initialized');
            return true;
        } catch (error) {
            console.error('‚ùå Webcam initialization failed:', error);
            if (error.name === 'NotAllowedError') {
                alert('Camera access denied. Please enable camera permissions to use facial monitoring.');
            } else if (error.name === 'NotFoundError') {
                alert('No camera found. Facial monitoring requires a webcam.');
            }
            return false;
        }
    }
    
    async startDetection(sessionId) {
        if (!this.modelsLoaded || !this.stream) {
            console.error('Models not loaded or webcam not initialized');
            return;
        }
        
        this.isRunning = true;
        console.log('üü¢ Facial detection started');
        
        this.detectionInterval = setInterval(async () => {
            if (!this.isRunning || !this.video.srcObject) return;
            
            try {
                const detections = await faceapi
                    .detectAllFaces(this.video, new faceapi.TinyFaceDetector())
                    .withFaceExpressions();
                
                if (detections.length > 0) {
                    const expressions = detections[0].expressions;
                    
                    // Find dominant emotion
                    const emotions = {
                        'happy': expressions.happy,
                        'sad': expressions.sad,
                        'angry': expressions.angry,
                        'fearful': expressions.fearful,
                        'disgusted': expressions.disgusted,
                        'neutral': expressions.neutral,
                        'surprised': expressions.surprised
                    };
                    
                    const emotion = Object.keys(emotions).reduce((a, b) =>
                        emotions[a] > emotions[b] ? a : b
                    );
                    const confidence = Math.max(...Object.values(expressions));
                    
                    // Update UI
                    document.getElementById('emotion-display').textContent = emotion.charAt(0).toUpperCase() + emotion.slice(1);
                    
                    // Send to backend
                    try {
                        await fetch('/api/analytics/affective/record-facial', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' },
                            body: JSON.stringify({
                                student_id: window.currentStudent?.id || 'anonymous',
                                session_id: sessionId,
                                emotion: emotion,
                                confidence: confidence
                            })
                        });
                    } catch (error) {
                        console.error('Error sending facial data:', error);
                    }
                } else {
                    document.getElementById('emotion-display').textContent = '--';
                }
            } catch (error) {
                console.error('Facial detection error:', error);
            }
        }, 500); // Detect every 500ms (2 FPS)
    }
    
    stopDetection() {
        if (this.detectionInterval) {
            clearInterval(this.detectionInterval);
        }
        this.isRunning = false;
        console.log('üî¥ Facial detection stopped');
    }
    
    async stop() {
        this.stopDetection();
        if (this.stream) {
            this.stream.getTracks().forEach(track => track.stop());
        }
    }
}

// Initialize facial capture
async function setupFacialMonitoring() {
    const checkbox = document.getElementById('facial-enabled');
    const container = document.getElementById('facial-monitoring-container');
    
    if (!checkbox) return; // Element not found
    
    // Load face-api.js library
    const script = document.createElement('script');
    script.async = true;
    script.src = 'https://cdn.jsdelivr.net/npm/@vladmandic/face-api/dist/face-api.js';
    script.onload = () => {
        console.log('‚úÖ Face-API library loaded');
        container.style.display = 'block';
    };
    document.head.appendChild(script);
    
    // Handle checkbox changes
    checkbox.addEventListener('change', async (e) => {
        if (e.target.checked) {
            // Enable facial monitoring
            facialCapture = new WebcamFacialCapture('webcam-video');
            const initialized = await facialCapture.initialize();
            
            if (initialized) {
                const sessionId = window.currentSession?.id || new Date().getTime().toString();
                facialCapture.startDetection(sessionId);
                facialMonitoringEnabled = true;
                console.log('‚úÖ Facial monitoring enabled');
            } else {
                e.target.checked = false;
                facialMonitoringEnabled = false;
            }
        } else {
            // Disable facial monitoring
            if (facialCapture) {
                await facialCapture.stop();
            }
            facialMonitoringEnabled = false;
            console.log('‚úÖ Facial monitoring disabled');
        }
    });
}

// Call this when the page loads
document.addEventListener('DOMContentLoaded', setupFacialMonitoring);

// Make sure to stop facial monitoring when test ends
function onTestEnd() {
    if (facialCapture && facialMonitoringEnabled) {
        facialCapture.stop();
        facialMonitoringEnabled = false;
    }
}
```

### Step 3: Add Script Tag to HTML
Add this to `frontend/index.html` before closing `</body>` tag:

```html
<script src="app.js"></script>
```

---

## NEW API ENDPOINTS ADDED

```
POST /api/analytics/affective/record-facial
‚îú‚îÄ‚îÄ Input: { emotion, confidence, student_id, session_id }
‚îî‚îÄ‚îÄ Output: { emotion, engagement_score, frustration_level }

GET /api/analytics/affective/facial-summary/<session_id>
‚îú‚îÄ‚îÄ Returns: Facial expression statistics for session
‚îî‚îÄ‚îÄ Output: { avg_engagement, primary_emotion, frames_analyzed }

GET /api/analytics/affective/facial-capabilities
‚îú‚îÄ‚îÄ Returns: Available facial detection features
‚îî‚îÄ‚îÄ Output: { providers, emotions, accuracy, privacy_level }

GET /api/analytics/affective/facial-privacy
‚îú‚îÄ‚îÄ Returns: Privacy and data handling information
‚îî‚îÄ‚îÄ Output: { data_location, retention, gdpr_compliant }
```

---

## QUICK TEST

### Test 1: Check Facial Capabilities (No Camera Needed)
```bash
curl http://localhost:5000/api/analytics/affective/facial-capabilities
```

Expected output:
```json
{
  "providers_available": ["face.js", "azure", "aws", "mediapipe"],
  "current_provider": "face.js",
  "emotions_supported": ["happy", "sad", "angry", ...],
  "accuracy": "85-90%",
  "privacy_level": "maximum"
}
```

### Test 2: Check Privacy Policy
```bash
curl http://localhost:5000/api/analytics/affective/facial-privacy
```

### Test 3: Test Facial Expression Recording
```bash
curl -X POST http://localhost:5000/api/analytics/affective/record-facial \
  -H "Content-Type: application/json" \
  -d '{
    "student_id": "student1",
    "session_id": "session1",
    "emotion": "happy",
    "confidence": 0.95
  }'
```

---

## BROWSER COMPATIBILITY

| Browser | Support | Notes |
|---------|---------|-------|
| Chrome  | ‚úÖ Yes  | Full support, recommended |
| Firefox | ‚úÖ Yes  | Full support |
| Safari  | ‚úÖ Yes  | Requires HTTPS in production |
| Edge    | ‚úÖ Yes  | Full support |
| Mobile  | ‚ö†Ô∏è Limited | Works but may need back camera |

---

## TROUBLESHOOTING

### Issue: "Camera access denied"
**Solution:** Check browser permissions. Click camera icon in address bar ‚Üí Allow

### Issue: "Face-API models fail to load"
**Solution:** Check internet connection (models loaded from CDN)

### Issue: "Face not detected"
**Solution:** Improve lighting, position face clearly in frame

### Issue: "Browser blocks camera"
**Solution:** Use HTTPS (required in production), or localhost for development

---

## IMPLEMENTATION TIMELINE

| Task | Time | Status |
|------|------|--------|
| Backend API endpoints | ‚úÖ DONE | 4 endpoints added |
| Facial integrator module | ‚úÖ DONE | facial_expression_api.py |
| Frontend HTML | ‚è≥ TODO | 5 min |
| Frontend JavaScript | ‚è≥ TODO | 10 min |
| Testing | ‚è≥ TODO | 5 min |

**Total: ~20 minutes to full integration**

---

## NEXT STEPS

1. **Add HTML** to `frontend/index.html` (Step 1 above)
2. **Add JavaScript** to `frontend/app.js` (Step 2 above)
3. **Test in browser**:
   - Open http://localhost:5000
   - Enable facial capture checkbox
   - Allow camera access when prompted
   - See emotion updates in real-time
4. **Check backend** logs for emotion data being received
5. **Test endpoint** with curl commands above

---

## FEATURES ENABLED

‚úÖ Real-time facial expression detection  
‚úÖ Emotion to engagement score mapping  
‚úÖ Privacy-first local processing  
‚úÖ Browser webcam integration  
‚úÖ Optional user opt-in  
‚úÖ Session-based tracking  
‚úÖ Frustration detection  
‚úÖ Attention monitoring  

---

## DOCUMENTATION

- Full guide: `FACIAL_API_IMPLEMENTATION_GUIDE.md`
- 3 integration approaches explained
- Privacy & challenges addressed
- Cost analysis provided
- Next-level upgrades documented

---

**Ready to deploy!** üöÄ
